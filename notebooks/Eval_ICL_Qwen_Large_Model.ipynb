{"cells":[{"cell_type":"markdown","metadata":{"id":"leQMoYJbtBLe"},"source":["# ICL Evaluation for Larger Qwen Models\n","\n","Purpose: Evaluate larger Qwen models on HumanEval and HumanEval+ using the optimal 5-shot prompt\n","\n","This notebook evaluates larger Qwen models (e.g., Qwen2.5-Coder-7B-Instruct or Qwen2.5-Coder-15B-Instruct) on:\n","- **HumanEval**: 164 Python problems\n","- **HumanEval+**: Extended version with more test cases\n","\n","**Using the best prompt configuration from MBPP selection: 5-shot**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFOo-cmBtBLm"},"outputs":[],"source":["%pip install transformers accelerate datasets tqdm sentencepiece human-eval evalplus --upgrade -q\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K4cCcX8StBLp","outputId":"886368d0-774f-4dd1-dd9d-50de1ddc03e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Config loaded\n","  SEED = 11667\n"]}],"source":["import os, re, json, math\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from human_eval.data import read_problems\n","from tqdm.auto import tqdm\n","\n","# Ensure math.comb is available (Python 3.8+)\n","if not hasattr(math, 'comb'):\n","    def comb(n, k):\n","        if k > n or k < 0:\n","            return 0\n","        if k == 0 or k == n:\n","            return 1\n","        k = min(k, n - k)\n","        result = 1\n","        for i in range(k):\n","            result = result * (n - i) // (i + 1)\n","        return result\n","    math.comb = comb\n","\n","SEED = 11667\n","print(\"âœ… Config loaded\")\n","print(f\"  SEED = {SEED}\")\n"]},{"cell_type":"markdown","metadata":{"id":"_Qh6MpXctBLq"},"source":["## Model Configuration\n","\n","**Change the model name here to evaluate different model sizes:**\n","- `Qwen/Qwen2.5-Coder-7B-Instruct` (7B parameters)\n","- `Qwen/Qwen2.5-Coder-15B-Instruct` (15B parameters)\n","- Or any other Qwen model you want to test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":106,"referenced_widgets":["b32e31f9d3444f9a873713ae8f8047a4","55b6ec1d5b1e495a92b59fda76a4444a","81755368bd3449f592be6e41976527e3","ab2dabc7d04e4beab0dfb585be6e11b0","1210acf903154befa5e41b911e2c1175","57daceee49fb4c8cac16210993ec328b","e03dc713f76348e9b5565537e8208d6f","22777023a0fc4151ab74d6c524ada16b","97797331ae1b43cfad0006870598473a","9b028468c710413598a0e3a45b4db93c","a48f8a5f21ac4f409f63138c9de20ace"]},"id":"eq4IMyL_tBLr","outputId":"a4a58965-582f-4cbd-b279-e58cc9c90522"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model: Qwen/Qwen2.5-Coder-7B-Instruct\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b32e31f9d3444f9a873713ae8f8047a4"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"]},{"output_type":"stream","name":"stdout","text":["âœ… Model loaded and set to eval()\n"]}],"source":["# ========== CONFIGURATION ==========\n","# Change this to the model you want to evaluate\n","MODEL_NAME = \"Qwen/Qwen2.5-Coder-7B-Instruct\"  # Change to 15B or other sizes as needed\n","\n","print(f\"Loading model: {MODEL_NAME}\")\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME,\n","    dtype=torch.float16,\n","    device_map=\"auto\",\n","    trust_remote_code=True\n",")\n","\n","model.eval()\n","print(\"âœ… Model loaded and set to eval()\")\n"]},{"cell_type":"markdown","metadata":{"id":"Ngtt7hnWtBLt"},"source":["## 5-Shot Prompt Configuration (Best from MBPP Selection)\n","\n","Using the optimal 5-shot prompt configuration that achieved 35.0% accuracy on MBPP.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZzBeLAfwtBLu","outputId":"5c578cca-1954-4f8c-9e9f-4cad18a055f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… 5-shot prompt configuration loaded\n"]}],"source":["# 5-shot pure code examples (best configuration from MBPP selection)\n","CODE_EXAMPLE_1 = \"\"\"def factorial(n):\n","    if n == 0 or n == 1:\n","        return 1\n","    return n * factorial(n - 1)\n","\n","\"\"\"\n","\n","CODE_EXAMPLE_2 = \"\"\"def is_palindrome(s):\n","    s = s.lower().replace(\" \", \"\")\n","    return s == s[::-1]\n","\n","\"\"\"\n","\n","CODE_EXAMPLE_3 = \"\"\"def fibonacci(n):\n","    if n <= 0:\n","        return []\n","    elif n == 1:\n","        return [0]\n","    elif n == 2:\n","        return [0, 1]\n","    seq = [0, 1]\n","    for i in range(2, n):\n","        seq.append(seq[-1] + seq[-2])\n","    return seq\n","\n","\"\"\"\n","\n","CODE_EXAMPLE_4 = \"\"\"def find_max(lst):\n","    if not lst:\n","        return None\n","    m = lst[0]\n","    for x in lst[1:]:\n","        if x > m:\n","            m = x\n","    return m\n","\n","\"\"\"\n","\n","CODE_EXAMPLE_5 = \"\"\"def reverse_list(lst):\n","    i, j = 0, len(lst) - 1\n","    while i < j:\n","        lst[i], lst[j] = lst[j], lst[i]\n","        i += 1\n","        j -= 1\n","    return lst\n","\n","\"\"\"\n","\n","ALL_CODE_EXAMPLES = [\n","    CODE_EXAMPLE_1,\n","    CODE_EXAMPLE_2,\n","    CODE_EXAMPLE_3,\n","    CODE_EXAMPLE_4,\n","    CODE_EXAMPLE_5,\n","]\n","\n","# Hard rule for prompt\n","HARD_RULE = (\n","    \"# You are a Python coding assistant.\\n\"\n","    \"# Only output valid Python code implementing the required function.\\n\"\n","    \"# Do NOT use markdown or ```.\\n\"\n","    \"# Do NOT print explanations or comments outside the function body.\\n\\n\"\n",")\n","\n","print(\"âœ… 5-shot prompt configuration loaded\")\n"]},{"cell_type":"markdown","metadata":{"id":"WsRH0Z_htBLv"},"source":["## Prompt Builder and Code Generation Functions\n","\n","**Note:** All generation parameters match the original notebook:\n","- `temperature=0.2`\n","- `top_p=0.95`\n","- `max_new_tokens=2048`\n","- `do_sample=True`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7hRIseGStBLv","outputId":"2b84e846-7113-493e-8c1d-460f1aeada0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Prompt builder and generation functions ready\n"]}],"source":["def get_signature(problem):\n","    \"\"\"Extract function signature from HumanEval problem.\"\"\"\n","    prompt_lines = problem[\"prompt\"].strip().split(\"\\n\")\n","    first_line = prompt_lines[0] if prompt_lines else \"\"\n","    if first_line and not first_line.rstrip().endswith(\":\"):\n","        first_line = first_line.rstrip() + \":\"\n","    return first_line + \"\\n\"\n","\n","def build_task_text_from_humaneval(problem):\n","    \"\"\"Extract task description from HumanEval problem docstring.\"\"\"\n","    prompt_lines = problem[\"prompt\"].strip().split(\"\\n\")\n","    if len(prompt_lines) > 1:\n","        docstring = \"\\n\".join(prompt_lines[1:])\n","        docstring = docstring.strip().strip('\"\"\"').strip(\"'''\").strip()\n","        return docstring\n","    return \"Complete the function\"\n","\n","def build_5shot_prompt(problem):\n","    \"\"\"Build 5-shot prompt for HumanEval problem.\"\"\"\n","    # 5-shot code prefix\n","    prefix = \"\".join(ALL_CODE_EXAMPLES)\n","\n","    # Task description\n","    task_text = build_task_text_from_humaneval(problem)\n","    one_line_task = task_text.replace(\"\\n\", \" \")\n","\n","    # Function signature\n","    sig = get_signature(problem)\n","\n","    # Combine\n","    prompt = (\n","        prefix + \"\\n\" +\n","        HARD_RULE +\n","        f\"# Task: {one_line_task}\\n\"\n","        f\"# Implement the following function to solve the task.\\n\\n\"\n","        f\"{sig}\"\n","    )\n","    return prompt\n","\n","def generate(model, tokenizer, prompt: str, max_new_tokens: int = 2048, num_samples: int = 1):\n","    \"\"\"Generate code from prompt.\"\"\"\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n","\n","    if num_samples == 1:\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                **inputs,\n","                max_new_tokens=max_new_tokens,\n","                do_sample=True,\n","                temperature=0.2,\n","                top_p=0.95,\n","                pad_token_id=tokenizer.eos_token_id,\n","            )\n","        return tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    else:\n","        generated_texts = []\n","        with torch.no_grad():\n","            outputs = model.generate(\n","                **inputs,\n","                max_new_tokens=max_new_tokens,\n","                do_sample=True,\n","                temperature=0.2,\n","                top_p=0.95,\n","                pad_token_id=tokenizer.eos_token_id,\n","                num_return_sequences=num_samples,\n","            )\n","        for output in outputs:\n","            generated_texts.append(tokenizer.decode(output, skip_special_tokens=True))\n","        return generated_texts\n","\n","def strip_prompt(full_output: str, prompt: str) -> str:\n","    \"\"\"Remove prompt from generated output.\"\"\"\n","    if prompt in full_output:\n","        return full_output.split(prompt, 1)[1].strip()\n","    return full_output.strip()\n","\n","def extract_code_from_markdown(text: str) -> str:\n","    \"\"\"Extract Python code from possibly-markdown output.\"\"\"\n","    if not isinstance(text, str):\n","        return \"\"\n","    s = text.strip()\n","\n","    # 1) Prefer fenced code blocks\n","    patterns = [\n","        r\"```python\\s*\\n(.*?)\\n```\",\n","        r\"```py\\s*\\n(.*?)\\n```\",\n","        r\"```\\s*\\n(.*?)\\n```\",\n","        r\"```python\\s*(.*?)```\",\n","        r\"```\\s*(.*?)```\",\n","    ]\n","    for pat in patterns:\n","        m = re.search(pat, s, flags=re.DOTALL | re.IGNORECASE)\n","        if m:\n","            code = m.group(1).strip()\n","            if len(code) > 0:\n","                s = code\n","                break\n","\n","    # 2) Remove leading markers\n","    s = re.sub(r\"^###\\s*(Instruction|Output|Response):\\s*\", \"\", s, flags=re.MULTILINE)\n","    s = re.sub(r\"^(Instruction|Output|Response):\\s*\", \"\", s, flags=re.MULTILINE)\n","\n","    # 3) Find def block\n","    m_def = re.search(r\"(def\\s+\\w+\\([^)]*\\):[\\s\\S]*)\", s)\n","    if m_def:\n","        s = m_def.group(1).strip()\n","\n","    # 4) Cut off at instruction markers\n","    s = s.split(\"```\")[0]\n","    s = s.split(\"### Instruction:\")[0]\n","    s = s.split(\"### Output:\")[0]\n","    s = s.split(\"### Response:\")[0]\n","\n","    return s.strip()\n","\n","print(\"âœ… Prompt builder and generation functions ready\")\n"]},{"cell_type":"markdown","metadata":{"id":"V1csxhdEtBLx"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":520,"referenced_widgets":["2208bbc6b44f4207b0539f8135caaa26","46154762886e43cdaef7e5b6f9f21949","bc85d71f276c4f1e875cf6046387ccd2","8b5065c1b7b841e49f0106de99a1f564","0de70a970f7d465bb4c495389b0579ec","ba477510d3d643ca8fd868f9babca626","f745aae335114522be10f935f0176b99","3a4f1949d40246d8ba02bb0c38c2ea6c","402f302864514ccda94ff7fa33ae30e3","6843a5e3572b48a3abdf05613bd1f456","d0f7baeb62c8498fa94f66beecf223ad"]},"id":"gB6LFheRtBLy","outputId":"1d7613e2-5c77-4de2-829f-680a15dc3193"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Loaded 164 HumanEval problems\n","ðŸš€ Evaluating HumanEval with 5-shot prompt\n","   Model: Qwen/Qwen2.5-Coder-7B-Instruct\n","   Total problems: 164\n","   Generating 10 samples per problem for pass@10 calculation\n"]},{"output_type":"display_data","data":{"text/plain":["HumanEval:   0%|          | 0/164 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2208bbc6b44f4207b0539f8135caaa26"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  Progress: 10/164 â€” pass@1=90.00%, pass@10=90.00%\n","  Progress: 20/164 â€” pass@1=70.00%, pass@10=70.00%\n","  Progress: 30/164 â€” pass@1=70.00%, pass@10=70.00%\n","  Progress: 40/164 â€” pass@1=57.50%, pass@10=57.50%\n","  Progress: 50/164 â€” pass@1=46.00%, pass@10=46.00%\n","  Progress: 60/164 â€” pass@1=40.00%, pass@10=40.00%\n","  Progress: 70/164 â€” pass@1=35.71%, pass@10=37.14%\n","  Progress: 80/164 â€” pass@1=32.50%, pass@10=33.75%\n","  Progress: 90/164 â€” pass@1=28.89%, pass@10=30.00%\n","  Progress: 100/164 â€” pass@1=26.00%, pass@10=27.00%\n","  Progress: 110/164 â€” pass@1=23.64%, pass@10=25.45%\n","  Progress: 120/164 â€” pass@1=22.50%, pass@10=25.83%\n","  Progress: 130/164 â€” pass@1=20.77%, pass@10=23.85%\n","  Progress: 140/164 â€” pass@1=19.29%, pass@10=22.14%\n","  Progress: 150/164 â€” pass@1=18.67%, pass@10=21.33%\n","  Progress: 160/164 â€” pass@1=17.50%, pass@10=20.00%\n","\n","âœ… HumanEval pass@1: 0.1707 (28/164)\n","âœ… HumanEval pass@10: 0.1951 (32/164)\n","ðŸ’¾ Saved HumanEval results to results/Qwen2.5_Coder_7B_Instruct_humaneval_5shot.json\n"]}],"source":["humaneval_problems = read_problems()\n","print(f\"âœ… Loaded {len(humaneval_problems)} HumanEval problems\")\n","\n","def check_humaneval(code: str, problem: dict) -> bool:\n","    \"\"\"Check HumanEval code correctness using exec().\"\"\"\n","    code_clean = extract_code_from_markdown(code)\n","    if not code_clean or len(code_clean) < 8:\n","        return False\n","\n","    prompt_sig = problem[\"prompt\"]\n","    test_code = problem[\"test\"]\n","\n","    if prompt_sig in code_clean:\n","        body = code_clean.replace(prompt_sig, \"\")\n","    else:\n","        body = code_clean\n","\n","    full = prompt_sig + body + \"\\n\" + test_code\n","    try:\n","        exec(full, {})\n","        return True\n","    except Exception:\n","        return False\n","\n","def eval_humaneval(model, tokenizer, calculate_pass10: bool = True):\n","    \"\"\"Evaluate HumanEval using 5-shot prompt.\"\"\"\n","    correct_pass1 = 0\n","    correct_pass10 = 0\n","    total = len(humaneval_problems)\n","    n_samples = 10 if calculate_pass10 else 1\n","\n","    print(f\"ðŸš€ Evaluating HumanEval with 5-shot prompt\")\n","    print(f\"   Model: {MODEL_NAME}\")\n","    print(f\"   Total problems: {total}\")\n","    if calculate_pass10:\n","        print(f\"   Generating {n_samples} samples per problem for pass@10 calculation\")\n","\n","    for i, (task_id, problem) in enumerate(tqdm(humaneval_problems.items(), desc=\"HumanEval\")):\n","        prompt = build_5shot_prompt(problem)\n","\n","        if calculate_pass10:\n","            full_outs = generate(model, tokenizer, prompt, max_new_tokens=2048, num_samples=n_samples)\n","            passed_samples = 0\n","            for full_out in full_outs:\n","                gen_part = strip_prompt(full_out, prompt)\n","                code = extract_code_from_markdown(gen_part)\n","                if check_humaneval(code, problem):\n","                    passed_samples += 1\n","\n","            # Pass@1: first sample passes\n","            if check_humaneval(extract_code_from_markdown(strip_prompt(full_outs[0], prompt)), problem):\n","                correct_pass1 += 1\n","\n","            # Pass@10: at least one sample passes\n","            if passed_samples > 0:\n","                correct_pass10 += 1\n","        else:\n","            full_out = generate(model, tokenizer, prompt, max_new_tokens=2048, num_samples=1)\n","            gen_part = strip_prompt(full_out, prompt)\n","            code = extract_code_from_markdown(gen_part)\n","            if check_humaneval(code, problem):\n","                correct_pass1 += 1\n","\n","        if (i + 1) % 10 == 0:\n","            current_acc1 = correct_pass1 / (i + 1)\n","            if calculate_pass10:\n","                current_acc10 = correct_pass10 / (i + 1)\n","                tqdm.write(f\"  Progress: {i+1}/{total} â€” pass@1={current_acc1:.2%}, pass@10={current_acc10:.2%}\")\n","            else:\n","                tqdm.write(f\"  Progress: {i+1}/{total} â€” pass@1={current_acc1:.2%}\")\n","\n","    acc_pass1 = correct_pass1 / total\n","    results = {\n","        \"pass@1\": acc_pass1,\n","        \"pass@1_correct\": correct_pass1,\n","        \"pass@1_total\": total,\n","    }\n","\n","    if calculate_pass10:\n","        acc_pass10 = correct_pass10 / total\n","        results[\"pass@10\"] = acc_pass10\n","        results[\"pass@10_correct\"] = correct_pass10\n","        results[\"pass@10_total\"] = total\n","        print(f\"\\nâœ… HumanEval pass@1: {acc_pass1:.4f} ({correct_pass1}/{total})\")\n","        print(f\"âœ… HumanEval pass@10: {acc_pass10:.4f} ({correct_pass10}/{total})\")\n","    else:\n","        print(f\"\\nâœ… HumanEval pass@1: {acc_pass1:.4f} ({correct_pass1}/{total})\")\n","\n","    return results\n","\n","# Run evaluation\n","humaneval_results = eval_humaneval(model, tokenizer, calculate_pass10=True)\n","\n","# Save results\n","os.makedirs(\"results\", exist_ok=True)\n","model_name_short = MODEL_NAME.split(\"/\")[-1].replace(\"-\", \"_\")\n","output_file = f\"results/{model_name_short}_humaneval_5shot.json\"\n","with open(output_file, \"w\") as f:\n","    json.dump({\n","        \"model\": MODEL_NAME,\n","        \"prompt_config\": \"5-shot (best from MBPP selection)\",\n","        \"pass@1\": float(humaneval_results[\"pass@1\"]),\n","        \"pass@1_correct\": int(humaneval_results[\"pass@1_correct\"]),\n","        \"pass@1_total\": int(humaneval_results[\"pass@1_total\"]),\n","        \"pass@10\": float(humaneval_results.get(\"pass@10\", 0)),\n","        \"pass@10_correct\": int(humaneval_results.get(\"pass@10_correct\", 0)),\n","        \"pass@10_total\": int(humaneval_results.get(\"pass@10_total\", 0)),\n","    }, f, indent=2)\n","print(f\"ðŸ’¾ Saved HumanEval results to {output_file}\")\n"]},{"cell_type":"code","source":["humaneval_problems = read_problems()\n","print(f\"âœ… Loaded {len(humaneval_problems)} HumanEval problems\")   # åœ¨notebookä¸­æ·»åŠ è°ƒè¯•ä»£ç \n","for i in range(5):\n","    problem = list(humaneval_problems.items())[i]\n","    prompt = build_5shot_prompt(problem[1])\n","    output = generate(model, tokenizer, prompt)\n","    code = extract_code_from_markdown(strip_prompt(output, prompt))\n","    print(f\"Problem {i}:\")\n","    print(f\"Generated code:\\n{code[:500]}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d7w1U1gfKYrc","outputId":"2db949dd-764e-4ea0-e1fc-0403b26cc21d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Loaded 164 HumanEval problems\n","Problem 0:\n","Generated code:\n","def has_close_elements(numbers: List[float], threshold: float) -> bool:\n","    for i in range(len(numbers)):\n","        for j in range(i + 1, len(numbers)):\n","            if abs(numbers[i] - numbers[j]) < threshold:\n","                return True\n","    return False\n","\n","Problem 1:\n","Generated code:\n","def separate_paren_groups(paren_string: str) -> List[str]:\n","    paren_string = paren_string.replace(' ', '')\n","    result = []\n","    current_group = []\n","    depth = 0\n","\n","    for char in paren_string:\n","        current_group.append(char)\n","        if char == '(':\n","            depth += 1\n","        elif char == ')':\n","            depth -= 1\n","            if depth == 0:\n","                result.append(''.join(current_group))\n","                current_group = []\n","\n","    return result\n","\n","Problem 2:\n","Generated code:\n","return number - int(number)\n","\n","Problem 3:\n","Generated code:\n","def below_zero(operations: List[int]) -> bool:\n","    balance = 0\n","    for operation in operations:\n","        balance += operation\n","        if balance < 0:\n","            return True\n","    return False\n","\n","Problem 4:\n","Generated code:\n","def mean_absolute_deviation(numbers: List[float]) -> float:\n","    mean = sum(numbers) / len(numbers)\n","    mad = sum(abs(x - mean) for x in numbers) / len(numbers)\n","    return mad\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDCAwhVttBL0"},"outputs":[],"source":["pip install evalplus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2YzwKv87tBL0"},"outputs":[],"source":["# Install evalplus if not available, then load HumanEval+\n","try:\n","    from evalplus.data import get_human_eval_plus\n","    humaneval_plus_problems = get_human_eval_plus()\n","    print(f\"âœ… Loaded {len(humaneval_plus_problems)} HumanEval+ problems\")\n","except ImportError:\n","    print(\"âš ï¸  evalplus not installed. Installing now...\")\n","    import subprocess\n","    import sys\n","    import importlib\n","    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"evalplus\", \"-q\"])\n","    print(\"âœ… evalplus installed, reloading...\")\n","    # Reload the module if it was partially loaded\n","    if 'evalplus' in sys.modules:\n","        del sys.modules['evalplus']\n","    try:\n","        # Re-attempt import from evalplus.data after install\n","        from evalplus.data import get_human_eval_plus\n","    except ImportError:\n","        # Fallback to direct submodule import if top-level still fails\n","        from evalplus.data.humaneval import get_human_eval_plus\n","    humaneval_plus_problems = get_human_eval_plus()\n","    print(f\"âœ… Loaded {len(humaneval_plus_problems)} HumanEval+ problems\")\n","except Exception as e:\n","    print(f\"âš ï¸  Failed to load HumanEval+: {e}\")\n","    print(\"   You can skip HumanEval+ evaluation if needed\")\n","    humaneval_plus_problems = None\n","\n","def check_humaneval_plus(code: str, problem: dict) -> bool:\n","    \"\"\"Check HumanEval+ code correctness using exec().\"\"\"\n","    code_clean = extract_code_from_markdown(code)\n","    if not code_clean or len(code_clean) < 8:\n","        return False\n","\n","    prompt_sig = problem[\"prompt\"]\n","    test_code = problem[\"test\"]\n","\n","    if prompt_sig in code_clean:\n","        body = code_clean.replace(prompt_sig, \"\")\n","    else:\n","        body = code_clean\n","\n","    full = prompt_sig + body + \"\\n\" + test_code\n","    try:\n","        exec(full, {})\n","        return True\n","    except Exception:\n","        return False\n","\n","def eval_humaneval_plus(model, tokenizer, calculate_pass10: bool = True):\n","    \"\"\"Evaluate HumanEval+ using 5-shot prompt.\"\"\"\n","    if humaneval_plus_problems is None:\n","        print(\"âŒ HumanEval+ not loaded. Skipping.\")\n","        return None\n","\n","    correct_pass1 = 0\n","    correct_pass10 = 0\n","    total = len(humaneval_plus_problems)\n","    n_samples = 10 if calculate_pass10 else 1\n","\n","    print(f\"ðŸš€ Evaluating HumanEval+ with 5-shot prompt\")\n","    print(f\"   Model: {MODEL_NAME}\")\n","    print(f\"   Total problems: {total}\")\n","    if calculate_pass10:\n","        print(f\"   Generating {n_samples} samples per problem for pass@10 calculation\")\n","\n","    for i, (task_id, problem) in enumerate(tqdm(humaneval_plus_problems.items(), desc=\"HumanEval+\")):\n","        prompt = build_5shot_prompt(problem)\n","\n","        if calculate_pass10:\n","            full_outs = generate(model, tokenizer, prompt, max_new_tokens=2048, num_samples=n_samples)\n","            passed_samples = 0\n","            for full_out in full_outs:\n","                gen_part = strip_prompt(full_out, prompt)\n","                code = extract_code_from_markdown(gen_part)\n","                if check_humaneval_plus(code, problem):\n","                    passed_samples += 1\n","\n","            # Pass@1: first sample passes\n","            if check_humaneval_plus(extract_code_from_markdown(strip_prompt(full_outs[0], prompt)), problem):\n","                correct_pass1 += 1\n","\n","            # Pass@10: at least one sample passes\n","            if passed_samples > 0:\n","                correct_pass10 += 1\n","        else:\n","            full_out = generate(model, tokenizer, prompt, max_new_tokens=2048, num_samples=1)\n","            gen_part = strip_prompt(full_out, prompt)\n","            code = extract_code_from_markdown(gen_part)\n","            if check_humaneval_plus(code, problem):\n","                correct_pass1 += 1\n","\n","        if (i + 1) % 10 == 0:\n","            current_acc1 = correct_pass1 / (i + 1)\n","            if calculate_pass10:\n","                current_acc10 = correct_pass10 / (i + 1)\n","                tqdm.write(f\"  Progress: {i+1}/{total} â€” pass@1={current_acc1:.2%}, pass@10={current_acc10:.2%}\")\n","            else:\n","                tqdm.write(f\"  Progress: {i+1}/{total} â€” pass@1={current_acc1:.2%}\")\n","\n","    acc_pass1 = correct_pass1 / total\n","    results = {\n","        \"pass@1\": acc_pass1,\n","        \"pass@1_correct\": correct_pass1,\n","        \"pass@1_total\": total,\n","    }\n","\n","    if calculate_pass10:\n","        acc_pass10 = correct_pass10 / total\n","        results[\"pass@10\"] = acc_pass10\n","        results[\"pass@10_correct\"] = correct_pass10\n","        results[\"pass@10_total\"] = total\n","        print(f\"\\nâœ… HumanEval+ pass@1: {acc_pass1:.4f} ({correct_pass1}/{total})\")\n","        print(f\"âœ… HumanEval+ pass@10: {acc_pass10:.4f} ({correct_pass10}/{total})\")\n","    else:\n","        print(f\"\\nâœ… HumanEval+ pass@1: {acc_pass1:.4f} ({correct_pass1}/{total})\")\n","\n","    return results\n","\n","# Run evaluation\n","humaneval_plus_results = eval_humaneval_plus(model, tokenizer, calculate_pass10=True)\n","\n","# Save results\n","if humaneval_plus_results is not None:\n","    os.makedirs(\"results\", exist_ok=True) # Ensure the directory exists\n","    model_name_short = MODEL_NAME.split(\"/\")[-1].replace(\"-\", \"_\")\n","    output_file = f\"results/{model_name_short}_humaneval_plus_5shot.json\"\n","    with open(output_file, \"w\") as f:\n","        json.dump({\n","            \"model\": MODEL_NAME,\n","            \"prompt_config\": \"5-shot (best from MBPP selection)\",\n","            \"pass@1\": float(humaneval_plus_results[\"pass@1\"]),\n","            \"pass@1_correct\": int(humaneval_plus_results[\"pass@1_correct\"]),\n","            \"pass@1_total\": int(humaneval_plus_results[\"pass@1_total\"]),\n","            \"pass@10\": float(humaneval_plus_results.get(\"pass@10\", 0)),\n","            \"pass@10_correct\": int(humaneval_plus_results.get(\"pass@10_correct\", 0)),\n","            \"pass@10_total\": int(humaneval_plus_results.get(\"pass@10_total\", 0))\n","        }, f, indent=2)\n","    print(f\"ðŸ’¾ Saved HumanEval+ results to {output_file}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iubPVh_utBL2","outputId":"9ab0d1fb-94c6-456c-d70c-2d08eca24292"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","ðŸ“Š Evaluation Results Summary\n","======================================================================\n","Model: Qwen/Qwen2.5-Coder-7B-Instruct\n","Prompt: 5-shot (best from MBPP selection)\n","\n","HumanEval:\n","  Pass@1:  0.1707 (28/164)\n","  Pass@10: 0.1951 (32/164)\n","\n","======================================================================\n"]}],"source":["# Print summary\n","print(\"=\"*70)\n","print(\"ðŸ“Š Evaluation Results Summary\")\n","print(\"=\"*70)\n","print(f\"Model: {MODEL_NAME}\")\n","print(f\"Prompt: 5-shot (best from MBPP selection)\")\n","print()\n","print(\"HumanEval:\")\n","print(f\"  Pass@1:  {humaneval_results['pass@1']:.4f} ({humaneval_results['pass@1_correct']}/{humaneval_results['pass@1_total']})\")\n","print(f\"  Pass@10: {humaneval_results.get('pass@10', 0):.4f} ({humaneval_results.get('pass@10_correct', 0)}/{humaneval_results.get('pass@10_total', 0)})\")\n","print()\n","if humaneval_plus_results is not None:\n","    print(\"HumanEval+:\")\n","    print(f\"  Pass@1:  {humaneval_plus_results['pass@1']:.4f} ({humaneval_plus_results['pass@1_correct']}/{humaneval_plus_results['pass@1_total']})\")\n","    print(f\"  Pass@10: {humaneval_plus_results.get('pass@10', 0):.4f} ({humaneval_plus_results.get('pass@10_correct', 0)}/{humaneval_plus_results.get('pass@10_total', 0)})\")\n","print(\"=\"*70)\n"]},{"cell_type":"markdown","metadata":{"id":"qBrj3sJltBL2"},"source":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2208bbc6b44f4207b0539f8135caaa26":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_46154762886e43cdaef7e5b6f9f21949","IPY_MODEL_bc85d71f276c4f1e875cf6046387ccd2","IPY_MODEL_8b5065c1b7b841e49f0106de99a1f564"],"layout":"IPY_MODEL_0de70a970f7d465bb4c495389b0579ec"}},"46154762886e43cdaef7e5b6f9f21949":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba477510d3d643ca8fd868f9babca626","placeholder":"â€‹","style":"IPY_MODEL_f745aae335114522be10f935f0176b99","value":"HumanEval:â€‡100%"}},"bc85d71f276c4f1e875cf6046387ccd2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a4f1949d40246d8ba02bb0c38c2ea6c","max":164,"min":0,"orientation":"horizontal","style":"IPY_MODEL_402f302864514ccda94ff7fa33ae30e3","value":164}},"8b5065c1b7b841e49f0106de99a1f564":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6843a5e3572b48a3abdf05613bd1f456","placeholder":"â€‹","style":"IPY_MODEL_d0f7baeb62c8498fa94f66beecf223ad","value":"â€‡164/164â€‡[30:08&lt;00:00,â€‡â€‡4.98s/it]"}},"0de70a970f7d465bb4c495389b0579ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba477510d3d643ca8fd868f9babca626":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f745aae335114522be10f935f0176b99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a4f1949d40246d8ba02bb0c38c2ea6c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"402f302864514ccda94ff7fa33ae30e3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6843a5e3572b48a3abdf05613bd1f456":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0f7baeb62c8498fa94f66beecf223ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b32e31f9d3444f9a873713ae8f8047a4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_55b6ec1d5b1e495a92b59fda76a4444a","IPY_MODEL_81755368bd3449f592be6e41976527e3","IPY_MODEL_ab2dabc7d04e4beab0dfb585be6e11b0"],"layout":"IPY_MODEL_1210acf903154befa5e41b911e2c1175"}},"55b6ec1d5b1e495a92b59fda76a4444a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57daceee49fb4c8cac16210993ec328b","placeholder":"â€‹","style":"IPY_MODEL_e03dc713f76348e9b5565537e8208d6f","value":"Loadingâ€‡checkpointâ€‡shards:â€‡100%"}},"81755368bd3449f592be6e41976527e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_22777023a0fc4151ab74d6c524ada16b","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_97797331ae1b43cfad0006870598473a","value":4}},"ab2dabc7d04e4beab0dfb585be6e11b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b028468c710413598a0e3a45b4db93c","placeholder":"â€‹","style":"IPY_MODEL_a48f8a5f21ac4f409f63138c9de20ace","value":"â€‡4/4â€‡[00:05&lt;00:00,â€‡â€‡1.10s/it]"}},"1210acf903154befa5e41b911e2c1175":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57daceee49fb4c8cac16210993ec328b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e03dc713f76348e9b5565537e8208d6f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22777023a0fc4151ab74d6c524ada16b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97797331ae1b43cfad0006870598473a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9b028468c710413598a0e3a45b4db93c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a48f8a5f21ac4f409f63138c9de20ace":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}